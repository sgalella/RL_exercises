{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01da22a7",
   "metadata": {},
   "source": [
    "# Exercise 3: Lunar Lander"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5c1129",
   "metadata": {},
   "source": [
    "In this notebook we will implement **Deep Q-Learning** (DQN) to solve the [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) environment.\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/lunar_lander.png\" alt=\"drawing\" width=\"500\"/></p>\n",
    "\n",
    "In this environment, we want a lunar lander to learn how to land. The environment is defined by:\n",
    "\n",
    "A state which is an 8-dimensional vector containing the lander's position (x, y), velocity (x, y), angle, and angular velocity, as well as the status of the left and right legs, whether they are touching the ground (1) or not (0).\n",
    "\n",
    "The lander can do four different actions:\n",
    "\n",
    "- 0: Do nothing\n",
    "- 1: Fire the left engine\n",
    "- 2: Fire the main engine\n",
    "- 3: Fire the right engine\n",
    "\n",
    "The lander will receive rewards according to multiple transitions. The reward will be increased if:\n",
    "- The lander is closer to the lander pad.\n",
    "- The lander moves slow.\n",
    "- Legs are in contact with the ground.\n",
    "- The lander lands safely (+100)\n",
    "\n",
    "The reward will decreased if:\n",
    "- The lander is far from the lander pad.\n",
    "- The lander moves fast.\n",
    "- The lander tilts (non-horizontal angle).\n",
    "- The side engines are firing.\n",
    "- The lander crashes (-100)\n",
    "\n",
    "If the lander scores 200 points at an episode, it is considered solved.\n",
    "\n",
    "The episode terminates if the lander crashes or gets outside the viewport. For more information, check the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93426725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import gymnasium as gym\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from IPython.display import Video\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b213ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_env = {'id': 'LunarLander-v3',\n",
    "            'render_mode': 'rgb_array'}\n",
    "\n",
    "env = gym.make(**dict_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5226a57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for reproducibility\n",
    "seed = 5678\n",
    "\n",
    "# This is the initial state of the environment.\n",
    "observation, info = env.reset(seed=seed)\n",
    "print(f'Initial observation: {observation}')\n",
    "print(f'Initial info: {info}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4516a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for the action space to ensure reproducibility\n",
    "seed = 5678\n",
    "observation, info = env.reset(seed=seed)\n",
    "env.action_space.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9e6f7a",
   "metadata": {},
   "source": [
    "Let's run one episode to see how well does the agent perform under a random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6cbdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "total_reward = 0\n",
    "frames = []\n",
    "\n",
    "while not done:\n",
    "    # Save the frame for the video\n",
    "    frame = env.render()\n",
    "    frames.append(frame)\n",
    "\n",
    "    # Sample a random action from the action space\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Take a step in the environment\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    # Add the reward to the total reward\n",
    "    total_reward += reward\n",
    "\n",
    "    # Check if the episode has ended\n",
    "    done = terminated or truncated\n",
    "\n",
    "frame = env.render()\n",
    "frames.append(frame)\n",
    "\n",
    "print('Episode finished. Total reward:', total_reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baa9664",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('videos'):\n",
    "    os.makedirs('videos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6d795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_video_path = os.path.join('videos', f'lunar_lander_random.mp4')\n",
    "imageio.mimsave(random_video_path, frames, fps=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23406d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(random_video_path, width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca88b877",
   "metadata": {},
   "source": [
    "Well, it's no surprise that the agent did not perform well under a random policy. The agent needs to learn how to land the lunar lander effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b307d7",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Define a Deep Q-Network (DQN)\n",
    "\n",
    "Let's implement a DQN so the agent can land the lunar lander. We use a DQN for several reasons.\n",
    "\n",
    "1. **Continuous state space**: The states are 8-dimensional and real-valued, so if we used a lookup table it would treat each combination as a new state and never generalize (due to float-point precision).\n",
    "2. **Nonlinear value functions**: Most problems have a nonlinear relationship between variables. We could use linear function approximation, but we would need to craft nonlinear features, which would be time consuming.\n",
    "3. **Automatic feature learning**: Using a DQN allows the agent to learn rich representations automatically from the states (inputs).\n",
    "\n",
    "The neural network will approximate the q-values using parameters $\\theta$:\n",
    "\n",
    "$$\n",
    "q(s, a) \\approx q_\\theta(s, a)\n",
    "$$\n",
    "\n",
    "In this first exercise, we are going to implement the `DeepQNetwork` class. We are going to use a neural network with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d47352",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(torch.nn.Module):\n",
    "    def __init__(self, state_size=8, action_size=4, hidden_size=64):\n",
    "        super().__init__()\n",
    "        # Use linear layers to create a simple feedforward neural network.\n",
    "        raise NotImplementedError('You need to implement the neural network architecture.')\n",
    "        self.layer1 = ...\n",
    "        self.layer2 = ...\n",
    "        self.layer3 = ...\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.layer1(state))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72be105d",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Double click to see the solution.</summary>\n",
    "\n",
    "```python\n",
    "class DeepQNetwork(torch.nn.Module):\n",
    "    def __init__(self, state_size=8, action_size=4, hidden_size=64):\n",
    "        super().__init__()\n",
    "        # Use linear layers to create a simple feedforward neural network.\n",
    "        self.layer1 = torch.nn.Linear(state_size, hidden_size)  # Input to hidden\n",
    "        self.layer2 = torch.nn.Linear(hidden_size, hidden_size)  # Hidden to hidden\n",
    "        self.layer3 = torch.nn.Linear(hidden_size, action_size)  # HIdden to output \n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.layer1(state))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0445a30a",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Implement experience replay \n",
    "\n",
    "One challenge when using DQN is that consecutive transitions are highly correlated, which can cause large oscillating updates during training, which can be very unstable. To mitigate this, we will use an experience **replay buffer** to store past experiences and sample from them during training:\n",
    "\n",
    "- We will store each observed transition $(s, a, r, s', \\text{done})$ in a fix-size buffer.\n",
    "- During training, we sample random mini-batches from this buffer, which will break correlations.\n",
    "- When the buffer exceeds its capacity, we will remove the oldest transitions in FIFO (first in, first out) order.\n",
    "\n",
    "in the next cell, we implement the `ReplayBuffer` class. You'll need to implement the `push` method which appends the transition to the buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628a78db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size=10000):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # Append the transition to the buffer.\n",
    "        # Each transition is a tuple (state, action, reward, next_state, done).\n",
    "        # Hint treat self.buffer as a python list\n",
    "        raise NotImplementedError('You need to implement the push method.')\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.stack(states), np.stack(actions), np.stack(rewards), np.stack(next_states), np.stack(dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910658ec",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Double click to see the solution.</summary>\n",
    "\n",
    "```python\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size=10000):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.stack(states), np.stack(actions), np.stack(rewards), np.stack(next_states), np.stack(dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5ebdc5",
   "metadata": {},
   "source": [
    "### Exercise 3.3: Implement the lunar lander agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2642a61",
   "metadata": {},
   "source": [
    "Next, we are going to implement the `LunarLanderAgent` class, which will handle the training and action selection for the Lunar Lander environment. This agent will use a Deep Q-Network (DQN) to learn how to land the lunar lander safely.\n",
    "\n",
    "We implemented an epsilon decay to calculate the epsilon at each episode. The epsilon decay encourages initial exploration but later exploitation in a nonlinear way. You can check the `get_epsilon` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6342f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LunarLanderAgent:\n",
    "    def __init__(self, state_dim, action_dim, gamma=0.99, lr=1e-4, start_epsilon=1.0,\n",
    "                 final_epsilon=0.1, epsilon_decay=0.995, batch_size=64, buffer_size=10000,\n",
    "                 device='cpu'):\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.start_epsilon = start_epsilon\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.device = device\n",
    "\n",
    "        # Define the Deep Q-Network model and add it to the device.\n",
    "        raise NotImplementedError('You need to define the Deep Q-Network model.')\n",
    "        self.model = ...\n",
    "        self.lr = lr\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        # Initialize the replay buffer we implemented before. Pass the buffer_size parameter.\n",
    "        raise NotImplementedError('You need to initialize the replay buffer.')\n",
    "        self.buffer = ...\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def select_action(self, state, episode):\n",
    "        if np.random.rand() < self.get_epsilon(episode):\n",
    "            return np.random.randint(0, self.action_dim)        \n",
    "        state = torch.from_numpy(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state)\n",
    "        return q_values.argmax().item()\n",
    "      \n",
    "    def get_epsilon(self, episode):\n",
    "        return max(self.final_epsilon, self.start_epsilon * (self.epsilon_decay ** episode))\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def update(self):\n",
    "        # Sample a batch of transitions from the replay buffer.\n",
    "        raise NotImplementedError('You need to sample from the buffer.')\n",
    "        states, actions, rewards, next_states, dones = ...\n",
    "        \n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).long().to(self.device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "        next_states = torch.from_numpy(next_states).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.array(dones).astype(np.uint8)).float().to(self.device)\n",
    "\n",
    "        q_values = self.model(states).gather(1, actions.unsqueeze(-1))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Pass the next states through the model to get the Q-values.\n",
    "            raise NotImplementedError('You need to implement the next state Q-value calculation.')\n",
    "            y = ...\n",
    "            # Get the maximum Q-value for the next states.\n",
    "            next_q_values = y.max(dim=1)[0].detach()\n",
    "            q_target = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        loss = self.criterion(q_values, q_target.unsqueeze(1))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c3263f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Double click to see the solution.</summary>\n",
    "\n",
    "```python\n",
    "class LunarLanderAgent:\n",
    "    def __init__(self, state_dim, action_dim, gamma=0.99, lr=1e-4, start_epsilon=1.0,\n",
    "                 final_epsilon=0.1, epsilon_decay=0.995, batch_size=64, buffer_size=10000,\n",
    "                 device='cpu'):\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.start_epsilon = start_epsilon\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.device = device\n",
    "\n",
    "        # Define the Deep Q-Network model and add it to the device.\n",
    "        self.model = DeepQNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.lr = lr\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        # Initialize the replay buffer we implemented before. Pass the buffer_size parameter.\n",
    "        self.buffer = ReplayBuffer(buffer_size=buffer_size)   \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def select_action(self, state, episode):\n",
    "        if np.random.rand() < self.get_epsilon(episode):\n",
    "            return np.random.randint(0, self.action_dim)        \n",
    "        state = torch.from_numpy(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state)\n",
    "        return q_values.argmax().item()\n",
    "      \n",
    "    def get_epsilon(self, episode):\n",
    "        return max(self.final_epsilon, self.start_epsilon * (self.epsilon_decay ** episode))\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def update(self):\n",
    "        # Sample a batch of transitions from the replay buffer.\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).long().to(self.device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "        next_states = torch.from_numpy(next_states).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.array(dones).astype(np.uint8)).float().to(self.device)\n",
    "\n",
    "        q_values = self.model(states).gather(1, actions.unsqueeze(-1))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Pass the next states through the model to get the Q-values.\n",
    "            y = self.model(next_states)\n",
    "            # Get the maximum Q-value for the next states.\n",
    "            next_q_values = y.max(dim=1)[0].detach()\n",
    "            q_target = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        loss = self.criterion(q_values, q_target.unsqueeze(1))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4d5e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 9876\n",
    "observation, info = env.reset(seed=seed)\n",
    "env.action_space.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ddc5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81157c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "agent = LunarLanderAgent(state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2e87da",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 2000\n",
    "\n",
    "rewards = []\n",
    "rewards_window = deque(maxlen=100)\n",
    "tqdm_bar = tqdm(range(1, num_episodes+1), desc=f'Episodes')\n",
    "\n",
    "for episode in tqdm_bar:\n",
    "    state, _ = env.reset(seed=episode)\n",
    "    total_reward = 0\n",
    "\n",
    "    while True:\n",
    "        action = agent.select_action(state, episode)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        agent.store_transition(state, action, reward, next_state, done)\n",
    "\n",
    "        if len(agent.buffer) >= agent.batch_size:\n",
    "            agent.update()\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    rewards.append(total_reward)\n",
    "    rewards_window.append(total_reward)\n",
    "    tqdm_bar.set_postfix_str(f'Avg. Score = {np.mean(rewards_window):.2f}')\n",
    "\n",
    "    if np.mean(rewards_window) >= 200:\n",
    "        print(f'\\nEnvironment solved in {episode} episodes!')\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd99ba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "average_filter = np.ones(window_size) / window_size\n",
    "\n",
    "plt.plot(np.convolve(rewards, average_filter, mode='valid'))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title(f'Moving Average of Rewards (size={window_size})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ec50b3",
   "metadata": {},
   "source": [
    "As you can see, the agent learns to play the game over time, and the average reward increases as the training progresses. The moving average smooths out the fluctuations in the rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558bca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "total_reward = 0\n",
    "frames = []\n",
    "\n",
    "state, _ = env.reset(seed=episode+1)\n",
    "\n",
    "while True:\n",
    "    frame = env.render()\n",
    "    frames.append(frame)\n",
    "    \n",
    "    action = agent.select_action(state, episode)\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "frame = env.render()\n",
    "frames.append(frame)\n",
    "\n",
    "print('Episode finished. Total reward:', total_reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b57983",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_video_path = os.path.join('videos', f'lunar_lander_trained.mp4')\n",
    "imageio.mimsave(random_video_path, frames, fps=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6ebeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(random_video_path, width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7061c9ba",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- In this notebook, we implemented a **Deep Q-Network (DQN)** agent to solve the LunarLander environment. The agent uses a neural network to approximate the Q-values for each action given a state. \n",
    "\n",
    "- To prevent instabilities during training, we used a **experience replay** to store transitions and sample batches for training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
