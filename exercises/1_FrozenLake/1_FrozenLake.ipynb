{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e3d3cf7",
   "metadata": {},
   "source": [
    "# Exercise 1: Frozen Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17912e77",
   "metadata": {},
   "source": [
    "For the following exercises, we will use [Gymnasium](https://gymnasium.farama.org/), a toolkit for developing and comparing reinforcement learning algorithms. It provides a wide variety of environments, including classic control tasks, Atari games, and robotics simulations. We will start with the **Frozen Lake** problem. You can read more about this environment problem from the documentation [FrozenLake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/).\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/frozen_lake.png\" alt=\"drawing\" width=\"400\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be00d22",
   "metadata": {},
   "source": [
    "Some information of the environment:\n",
    "\n",
    "- The **environment** is an 8×8 frozen grid, giving us a total of 64 states. We start in the top-left corner, and our goal—the keys—is waiting in the bottom-right corner.\n",
    "\n",
    "- We can move in four directions: **left** (action 0), **down** (action 1), **right** (action 2), and **up** (action 3). If we try to step outside the boundaries of the grid, we simply stay in place (no falling off the map here).\n",
    "\n",
    "- But not all tiles are safe. Some of them hide **holes** in the ice. Step into one, and it’s game over. Our job is to navigate to the goal without falling in.\n",
    "\n",
    "- Reaching the goal gives us a reward of +1. Every other move worth zero. So we need to find a smart way to reach the goal efficiently and safely.\n",
    "\n",
    "- The ice can be slippery, which means that we will move in the desired direction with probability $1/3$. With probability $2/3$ we will move in a direction perpendicular to our intended direction (each with probability $1/3$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44786356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gymnasium as gym\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Video\n",
    "\n",
    "# We use these functions to plot the state values and the policy\n",
    "from utils import plot_values, plot_policy_and_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efec1b8",
   "metadata": {},
   "source": [
    "We will load the environment and set the variable `is_slippery` to `True` to make the environment slippery. This means that the agent will slip on the ice, making it more challenging to navigate the frozen lake.\n",
    "You can come back later and see how results change when you set `is_slippery` to `False` to see how transition dynamics change the optimal policy to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644c9fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_slippery = True  # Set to False for a deterministic environment\n",
    "\n",
    "# Here we define the environment for the FrozenLake task.\n",
    "dict_env = {'id': 'FrozenLake-v1',\n",
    "            'map_name': '8x8',\n",
    "            'render_mode': 'human',\n",
    "            'is_slippery': is_slippery,\n",
    "            'render_mode': 'rgb_array'\n",
    "            }\n",
    "\n",
    "env = gym.make(**dict_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d56b0e",
   "metadata": {},
   "source": [
    "Let's check some variables of the environment by inspecting the `env` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d23ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the number of states and actions in the environment.\n",
    "states = env.observation_space.n\n",
    "actions = env.action_space.n\n",
    "print(f'States: {states}, Actions: {actions}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f91355",
   "metadata": {},
   "source": [
    "We will set the seed for reproducibility and initialize the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0c2a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for reproducibility\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617bd1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the initial state of the environment.\n",
    "observation, info = env.reset(seed=seed)\n",
    "env.action_space.seed(seed)\n",
    "print(f'Initial observation: {observation}')\n",
    "print(f'Initial info: {info}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1856c7",
   "metadata": {},
   "source": [
    "The initial observation is the state we state at (top-left corner). The probability of starting from this state is 1. Now, Let's run one episode following a random policy to see how the agent performs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6185ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "total_reward = 0\n",
    "frames = []\n",
    "\n",
    "while not done:\n",
    "    # Save the frame for the video\n",
    "    frame = env.render()\n",
    "    frames.append(frame)\n",
    "\n",
    "    # Sample a random action from the action space\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Take a step in the environment\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    # Add the reward to the total reward\n",
    "    total_reward += reward\n",
    "\n",
    "    # Check if the episode has ended\n",
    "    done = terminated or truncated\n",
    "\n",
    "frame = env.render()\n",
    "frames.append(frame)\n",
    "\n",
    "print(\"Episode finished. Total reward:\", total_reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74aeaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('videos'):\n",
    "    os.makedirs('videos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f50c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_video_path = os.path.join('videos', f'frozenlake_random_slippery_{is_slippery}.mp4')\n",
    "imageio.mimsave(random_video_path, frames, fps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aaf53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(random_video_path, width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcddc86",
   "metadata": {},
   "source": [
    "Well, the random agent didn’t do very well, but that’s expected. It’s simply taking random actions without any strategy. Let’s try to come up with a smarter policy to help us to get to the goal!\n",
    "\n",
    "Remember from the lecture about the **Bellman expectation** can be written as:\n",
    "\n",
    "$$\n",
    "v_\\pi (s) = \\sum_{a \\in A}  \\pi(a | s) \\sum_{s' \\in S} \\sum_{r \\in R}P(s', r | s, a)[r + \\gamma v_\\pi (s')],\n",
    "$$\n",
    "\n",
    "since rewards are deterministic we can simplify the expression to:\n",
    "\n",
    "$$\n",
    "v_\\pi (s) = \\sum_{a \\in A}  \\pi(a | s) \\sum_{s' \\in S} P(s'| s, a)[R(s, a, s') + \\gamma v_\\pi (s')],\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81850a45",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Implement policy evaluation\n",
    "\n",
    "The first exercise consists in implementing a policy evaluation algorithm to estimate the value function of a given policy. \n",
    "\n",
    "We will use an iterative method, to evaluate our policy. The idea is the following: we will calculate the value function sequentially for each state. Once we finish we will repeat this step until convergence. Since the Bellman equations are convergent mappings, we are guaranteed to converge. We could alternative solve the linear system of equations.\n",
    "\n",
    "We need to fill the following function that takes a policy and an env object, together with a discount factor and maximum number of iterations and it returns the value function for the different states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea630b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, env, discount_factor=0.9, max_iterations=1000, theta=1e-6):\n",
    "    P = env.unwrapped.P\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    V = np.zeros(n_states)\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        delta = 0\n",
    "        new_V = np.copy(V)\n",
    "        for s in range(n_states):\n",
    "            v = 0\n",
    "            for a in range(n_actions):\n",
    "                for prob, next_state, reward, _ in P[s][a]:\n",
    "                    raise NotImplementedError(\"Comment out this line and implement the policy evaluation algorithm.\")\n",
    "                    v += ...\n",
    "            new_V[s] = v\n",
    "            delta = max(delta, abs(V[s] - v))\n",
    "\n",
    "        V = new_V\n",
    "\n",
    "        if delta < theta:\n",
    "            print(f\"Converged after {iteration + 1} sweeps.\")\n",
    "            break\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bf0e87",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Double click to see the solution.</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "def policy_evaluation(policy, env, discount_factor=0.9, max_iterations=1000, theta=1e-6):\n",
    "    P = env.unwrapped.P\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    V = np.zeros(n_states)\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        delta = 0\n",
    "        new_V = np.copy(V)\n",
    "        for s in range(n_states):\n",
    "            v = 0\n",
    "            for a in range(n_actions):\n",
    "                for prob, next_state, reward, _ in P[s][a]:\n",
    "                    v += policy[s, a] * prob * (reward + discount_factor * V[next_state])\n",
    "                    \n",
    "            new_V[s] = v\n",
    "            delta = max(delta, abs(V[s] - v))\n",
    "\n",
    "        V = new_V\n",
    "\n",
    "        if delta < theta:\n",
    "            print(f\"Converged after {iteration + 1} sweeps.\")\n",
    "            break\n",
    "\n",
    "    return V\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f17110",
   "metadata": {},
   "source": [
    "Let's evaluate a random policy to see how the value function looks like. We will use a uniform random policy, where each action has equal probability in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89047447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy is a matrix where each row corresponds to a state and each column corresponds to an action.\n",
    "# The value at policy[s, a] is the probability of taking action a in state s.\n",
    "policy = np.ones((states, actions)) / actions\n",
    "\n",
    "# print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72732501",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_pi = policy_evaluation(policy, env.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a321ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(7, 7), constrained_layout=True)\n",
    "\n",
    "ax[0].imshow(frames[0])\n",
    "ax[0].axis('off')\n",
    "plot_values(V_pi, env.env, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb7f0e6",
   "metadata": {},
   "source": [
    "### Exercise 1.2 Implement policy improvement\n",
    "\n",
    "Our goal is now to obtain a better policy by improving the current one using the value function we just computed. The policy improvement step involves updating the policy to choose actions that maximize the expected value based on the current value function.\n",
    "\n",
    "To do so, we will use the state values we found, and for each state, we will update the policy to choose the action that maximizes the expected value. This is done by iterating over all states and actions, calculating the expected value for each action, and then updating the policy to favor the action with the highest expected value.\n",
    "\n",
    "$$\n",
    "q_\\pi (s, a) = \\sum_{s' \\in S} \\sum_{r \\in R} P(s', r | s, a) (r + \\gamma \\sum_{a' \\in A} \\pi(a'|s')v(s'))\n",
    "$$\n",
    "\n",
    "again, since the rewards are deterministic:\n",
    "\n",
    "$$\n",
    "q_\\pi (s, a) = \\sum_{s' \\in S} P(s'| s, a) (R(s, a, s') + \\gamma \\sum_{a' \\in A} \\pi(a'|s')v(s'))\n",
    "$$\n",
    "\n",
    "Then, we can find a new policy:\n",
    "\n",
    "$$\n",
    "\\pi' (s) = \\arg \\max_a q_\\pi(s, a)\n",
    "$$\n",
    "\n",
    "We repeat these steps until the policy stops changing (i.e., it converges). Each update improves the policy, so we get a sequence:\n",
    "\n",
    "$$\n",
    "\\pi_0 < \\pi_1 < ... < \\pi_N = \\pi^*,\n",
    "$$\n",
    "\n",
    "where $\\pi^*$ is the **optimal policy**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4625c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(V, env, discount_factor=0.9):\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "    P = env.unwrapped.P\n",
    "\n",
    "    policy = np.zeros((num_states, num_actions))\n",
    "\n",
    "    for s in range(num_states):\n",
    "        action_values = np.zeros(num_actions)\n",
    "\n",
    "        for a in range(num_actions):\n",
    "            for p, s_, r, _ in P[s][a]:\n",
    "                raise NotImplementedError(\"Comment out this line and implement the policy evaluation algorithm.\")\n",
    "                action_values[a] += ...\n",
    "\n",
    "        # Greedy action (or break ties arbitrarily)\n",
    "        best_action = np.argmax(action_values)\n",
    "\n",
    "        # Deterministic policy: all prob on best_action\n",
    "        policy[s, best_action] = 1.0\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6827c89e",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Double click to see the solution.</summary>\n",
    "\n",
    "```python\n",
    "def policy_improvement(V, env, discount_factor=0.9):\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "    P = env.unwrapped.P\n",
    "\n",
    "    policy = np.zeros((num_states, num_actions))\n",
    "\n",
    "    for s in range(num_states):\n",
    "        action_values = np.zeros(num_actions)\n",
    "\n",
    "        for a in range(num_actions):\n",
    "            for p, s_, r, _ in P[s][a]:\n",
    "                action_values[a] += p * (r + discount_factor * V[s_])\n",
    "\n",
    "        # Greedy action (or break ties arbitrarily)\n",
    "        best_action = np.argmax(action_values)\n",
    "\n",
    "        # Deterministic policy: all prob on best_action\n",
    "        policy[s, best_action] = 1.0\n",
    "\n",
    "    return policy\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315194a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = policy_improvement(V_pi, env.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c7c005",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(7, 7), constrained_layout=True)\n",
    "\n",
    "ax[0].imshow(frames[0])\n",
    "ax[0].axis('off')\n",
    "plot_policy_and_values(V_pi, policy, env, ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad1324e",
   "metadata": {},
   "source": [
    "**Question**: What is the policy to follow in the states next to the holes? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe93d11c",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Double click to see the solution.</summary>\n",
    "\n",
    "The agent will learn that is safer to move in the perpendicular direction to the intended direction, which succeeds with probability 2/3, rather than moving in the intended direction, which succeeds with probability 1/3. This is because the agent can slip and fall into a hole if it moves in the intended direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc5d5d2",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Implement policy iteration\n",
    "\n",
    "Now it's time to implement the **policy iteration** algorithm. The policy iteration algorithm consists of two main steps: policy evaluation and policy improvement. We will iterate between these two steps until the policy converges.\n",
    "\n",
    "Use the previously defined `policy_evaluation` and `policy_improvement` functions to implement the policy iteration algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181ac37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, discount_factor=0.9, theta=1e-6, max_iterations=100):\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    # Start with uniform random policy\n",
    "    policy = np.ones((n_states, n_actions)) / n_actions\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        raise NotImplementedError(\"Comment out this line and and implement the policy iteration algorithm.\")\n",
    "        V = ...\n",
    "        new_policy = ...\n",
    "\n",
    "        if np.array_equal(policy, new_policy):\n",
    "            print(f\"Policy converged after {i + 1} iterations.\")\n",
    "            break\n",
    "\n",
    "        policy = new_policy\n",
    "\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6f0b6a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Double click to see the solution.</summary>\n",
    "\n",
    "```python\n",
    "def policy_iteration(env, discount_factor=0.9, theta=1e-6, max_iterations=100):\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    # Start with uniform random policy\n",
    "    policy = np.ones((n_states, n_actions)) / n_actions\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        V = policy_evaluation(policy, env, discount_factor=discount_factor, theta=theta)\n",
    "        new_policy = policy_improvement(V, env, discount_factor=discount_factor)\n",
    "\n",
    "        if np.array_equal(policy, new_policy):\n",
    "            print(f\"Policy converged after {i + 1} iterations.\")\n",
    "            break\n",
    "\n",
    "        policy = new_policy\n",
    "\n",
    "    return policy, V\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958f841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy, V_pi = policy_iteration(env.env, discount_factor=0.9, theta=1e-6, max_iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad82eabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(7, 7), constrained_layout=True)\n",
    "\n",
    "ax[0].imshow(frames[0])\n",
    "ax[0].axis('off')\n",
    "\n",
    "plot_policy_and_values(V_pi, policy, env, ax[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82abb3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset(seed=seed)\n",
    "env.action_space.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e47d277",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "total_reward = 0\n",
    "frames = []\n",
    "\n",
    "while not done:\n",
    "    # Save the frame for the video\n",
    "    frame = env.render()\n",
    "    frames.append(frame)\n",
    "\n",
    "    # Sample an action from the optimal policy\n",
    "    action = np.argmax(optimal_policy[observation])\n",
    "    \n",
    "    # Take a step in the environment\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    # Add the reward to the total reward\n",
    "    total_reward += reward\n",
    "\n",
    "    # Check if the episode has ended\n",
    "    done = terminated or truncated\n",
    "\n",
    "frame = env.render()\n",
    "frames.append(frame)\n",
    "\n",
    "print(\"Episode finished. Total reward:\", total_reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4f4e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_video_path = os.path.join('videos', f'frozenlake_optimal_slippery_{is_slippery}.mp4')\n",
    "imageio.mimsave(optimal_video_path, frames, fps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f818a7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(optimal_video_path, width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7762dee",
   "metadata": {},
   "source": [
    "We have implemented policy evaluation and policy iteration algorithms to find the optimal policy for the FrozenLake environment. We have implemented an iterative policy evaluation algorithm that computes the value function for a given policy, and then used this to extract the optimal policy.\n",
    "\n",
    "**Question**: Go back to the beginning of the notebook and change `is_slippery` to `False`. How does the optimal policy change? How do the state values following the optimal policy change?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00538298",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Double click to see the solution.</summary>\n",
    "\n",
    "The optimal policy differs between the two settings. When `is_slippery=True`, the agent’s moves become stochastic, so it often chooses actions perpendicular to the intended direction (which succeed with probability 2/3) to avoid accidentally slipping into a hole. You’ll also notice that, under this policy, states far from the goal have values near zero—because reaching the goal requires many risky steps, each discounted by γ.\n",
    "On the other hand, when `is_slippery=False`, transitions are deterministic and the agent simply follows the shortest safe path to the goal. As a result, even the initial states attain high values, since the agent can reliably reach the goal in only a few steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d899e9",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- Our objective is to learn a policy $\\pi$ (a way of behaving) that maximizes the reward over time, the **return**.\n",
    "\n",
    "- First we need to know how valuable are certain states and actions. To do so we have two different **value functions**:\n",
    "\n",
    "    - The **state-value** function $v_{\\pi}(s)$ tells  us the expected reward we will get if we start in a given state $s$ and then follow policy $\\pi$.\n",
    "    - The **action-value** function $q_{\\pi}(s, a)$ tells us the expected reward we will get if we start in a give state $s$, select a given action $a$, and then follow a given policy $\\pi$.\n",
    "\n",
    "- We can use the **Bellman expectation** equations for **policy evaluation** to calculate the **state-values** or the **action-values** of a MDP. These equations are linear, so can be solved exactly for small MDPs. However, for large MDPs, we use iterative methods, as we did for the Frozen Lake problem\n",
    "\n",
    "- We can extract a policy after **policy evaluation** by acting greedily, that is, to select the actions that lead us to better states. This is step is known as **policy improvement**.\n",
    "\n",
    "- To get the optimal policy, we can use the **Bellman optimality** equations, to calculate the optimal **state-values** or the optimal **action-values**. If we knew them, we only need to act greedily to find the optimal policy. However, these equations are nonlinear and there's no fast way of solving them.\n",
    "\n",
    "- Alternatively, we can do **policy iteration**, which consists on iterating **policy evaluation** and **policy improvement**. We are guaranteed to converge to a **optimal policy**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
